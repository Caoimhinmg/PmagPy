{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do basic imports and unpack McMurdo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nebula/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/__init__.py:1350: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got full_df\n",
      "got parsed_df\n",
      "got full_df\n",
      "got parsed_df\n",
      "-I- Importing controlled vocabularies from https://earthref.org\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from pmagpy import ipmag\n",
    "#reload(ipmag)\n",
    "from pmagpy import pmag\n",
    "from pmagpy import new_builder as nb\n",
    "from programs import data_model3\n",
    "reload(data_model3)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pmagpy.new_builder import Contribution\n",
    "from pmagpy import validate_upload3 as vu3\n",
    "\n",
    "import pmagpy.controlled_vocabularies3 as cv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mess up the Megiddo contribution a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-W- No such file: /Users/nebula/Python/PmagPy/3_0/Megiddo/images.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "measurement\n",
       "mgh05a01:LP-PI-TRM0     mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM1     mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM2     mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM3     mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM4     mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM5     mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM6     mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM7     mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM8     mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM9     mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM10    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM11    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM12    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM13    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM14    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM15    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM16    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM17    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM18    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM19    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM20    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM21    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM22    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM23    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM24    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM25    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM26    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM27    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM28    mgh05a01:LP-PI-TRM\n",
       "mgh05a01:LP-PI-TRM29    mgh05a01:LP-PI-TRM\n",
       "                               ...        \n",
       "mgh03g07:LP-AN-TRM3     mgh03g07:LP-AN-TRM\n",
       "mgh03g07:LP-AN-TRM4     mgh03g07:LP-AN-TRM\n",
       "mgh03g07:LP-AN-TRM5     mgh03g07:LP-AN-TRM\n",
       "mgh03g07:LP-AN-TRM6     mgh03g07:LP-AN-TRM\n",
       "mgh03g07:LP-AN-TRM7     mgh03g07:LP-AN-TRM\n",
       "mgh03g07:LP-AN-TRM8     mgh03g07:LP-AN-TRM\n",
       "mgh03b06:LP-AN-TRM1     mgh03b06:LP-AN-TRM\n",
       "mgh03b06:LP-AN-TRM2     mgh03b06:LP-AN-TRM\n",
       "mgh03b06:LP-AN-TRM3     mgh03b06:LP-AN-TRM\n",
       "mgh03b06:LP-AN-TRM4     mgh03b06:LP-AN-TRM\n",
       "mgh03b06:LP-AN-TRM5     mgh03b06:LP-AN-TRM\n",
       "mgh03b06:LP-AN-TRM6     mgh03b06:LP-AN-TRM\n",
       "mgh03b06:LP-AN-TRM7     mgh03b06:LP-AN-TRM\n",
       "mgh03b06:LP-AN-TRM8     mgh03b06:LP-AN-TRM\n",
       "mgh03h07:LP-AN-TRM1     mgh03h07:LP-AN-TRM\n",
       "mgh03h07:LP-AN-TRM2     mgh03h07:LP-AN-TRM\n",
       "mgh03h07:LP-AN-TRM3     mgh03h07:LP-AN-TRM\n",
       "mgh03h07:LP-AN-TRM4     mgh03h07:LP-AN-TRM\n",
       "mgh03h07:LP-AN-TRM5     mgh03h07:LP-AN-TRM\n",
       "mgh03h07:LP-AN-TRM6     mgh03h07:LP-AN-TRM\n",
       "mgh03h07:LP-AN-TRM7     mgh03h07:LP-AN-TRM\n",
       "mgh03h07:LP-AN-TRM8     mgh03h07:LP-AN-TRM\n",
       "mgh03h08:LP-AN-TRM1     mgh03h08:LP-AN-TRM\n",
       "mgh03h08:LP-AN-TRM2     mgh03h08:LP-AN-TRM\n",
       "mgh03h08:LP-AN-TRM3     mgh03h08:LP-AN-TRM\n",
       "mgh03h08:LP-AN-TRM4     mgh03h08:LP-AN-TRM\n",
       "mgh03h08:LP-AN-TRM5     mgh03h08:LP-AN-TRM\n",
       "mgh03h08:LP-AN-TRM6     mgh03h08:LP-AN-TRM\n",
       "mgh03h08:LP-AN-TRM7     mgh03h08:LP-AN-TRM\n",
       "mgh03h08:LP-AN-TRM8     mgh03h08:LP-AN-TRM\n",
       "Name: experiment, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = os.path.join(os.getcwd(), '3_0', 'Megiddo')\n",
    "con = Contribution(dir_path)\n",
    "\n",
    "loc_dm = con.tables['locations'].data_model.dm['locations']\n",
    "loc_df = con.tables['locations'].df\n",
    "site_dm = con.tables['sites'].data_model.dm['sites']\n",
    "site_df = con.tables['sites'].df\n",
    "samp_df = con.tables['samples'].df\n",
    "samp_dm = con.tables['samples'].data_model.dm['samples']\n",
    "spec_df = con.tables['specimens'].df\n",
    "spec_dm = con.tables['specimens'].data_model.dm['specimens']\n",
    "age_df = con.tables['ages'].df\n",
    "age_dm = con.tables['ages'].data_model.dm['ages']\n",
    "meas_df = con.tables['measurements'].df\n",
    "meas_dm = con.tables['measurements'].data_model.dm['measurements']\n",
    "cont_df = con.tables['contribution'].df\n",
    "cont_dm = con.tables['contribution'].data_model.dm['contribution']\n",
    "crit_df = con.tables['criteria'].df\n",
    "crit_dm = con.tables['criteria'].data_model.dm['criteria']\n",
    "\n",
    "\n",
    "current_con = con\n",
    "\n",
    "# mess up some validations for locations\n",
    "loc_df.loc['Tel Hazor', 'lat_s'] = 400.\n",
    "loc_df['dir_inc'] = 5\n",
    "loc_df.loc['Tel Hazor', 'lat_n'] = 'hello'\n",
    "loc_df.loc[:, 'lithologies'] = [\"Agate:Basalt\", \"Basalt:random\"]\n",
    "#current_con.tables.pop('sites')\n",
    "\n",
    "# mess up some validations for sites\n",
    "site_df.pop('age')\n",
    "#site_df['dir_tilt_correction'] = 1\n",
    "site_df.iloc[2, list(site_df.columns).index('dir_tilt_correction')] = 'not a number'\n",
    "site_df.iloc[0, list(site_df.columns).index('lithologies')] = \"Angrite:Basalt\"\n",
    "site_df.iloc[1, list(site_df.columns).index('lithologies')] = \"angrite : basalt\"\n",
    "\n",
    "# mess up some validations for ages\n",
    "age_df.ix[1]['age'] = 'a string'\n",
    "age_df.ix[1]['site'] = 'fake site'\n",
    "age_df.ix[1]['age_low'] = 1000000000000.\n",
    "age_df.pop('citations')\n",
    "\n",
    "# mess up some validations for samples\n",
    "samp_df.pop('citations')\n",
    "samp_df.iloc[0].lon = 600.\n",
    "samp_df.iloc[0].age = \"another string\"\n",
    "samp_df.iloc[0].lat = \"stringy\"\n",
    "samp_df.iloc[1].lat = 'hello'\n",
    "samp_df.iloc[2].specimens = \"hz05a2:fake\"\n",
    "samp_df.iloc[3].specimens = \"fake : hz05a1\"\n",
    "samp_df.iloc[5].specimens = 'fake_specimen'\n",
    "samp_df.iloc[7].site = 'fake_site'\n",
    "samp_df.iloc[0].cooling_rate = 'a string'\n",
    "\n",
    "# mess up some validations for measurements\n",
    "meas_df.loc['mgh05a01:LP-PI-TRM1', 'magn_moment'] = 2\n",
    "meas_df.loc['mgh05a01:LP-PI-TRM1', 'specimen'] = \"fake_specimen\"\n",
    "meas_df.pop('experiment')\n",
    "\n",
    "#current_df.head()\n",
    "#current_df.head()\n",
    "#age_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import pmagpy.controlled_vocabularies3 as cv\n",
    "#reload(cv)\n",
    "#vocab = cv.Vocabulary()\n",
    "#vocabulary, possible_vocabulary = vocab.get_controlled_vocabularies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## validation functions\n",
    "\n",
    "\n",
    "# need to add requiredOneInGroup\n",
    "\n",
    "# check that values pass validation\n",
    "# validation checks to add:\n",
    "# sv (suggested vocab)\n",
    "# requiredOneInGroup\n",
    "# requiredUnlessSynthetic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step, here is what the validate_table function does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citations</th>\n",
       "      <th>lat_n</th>\n",
       "      <th>lat_s</th>\n",
       "      <th>location</th>\n",
       "      <th>location_type</th>\n",
       "      <th>lon_e</th>\n",
       "      <th>lon_w</th>\n",
       "      <th>dir_inc</th>\n",
       "      <th>lithologies</th>\n",
       "      <th>presence_pass_age_requiredUnless</th>\n",
       "      <th>presence_pass_age_high_requiredUnless</th>\n",
       "      <th>presence_pass_age_low_requiredUnless</th>\n",
       "      <th>presence_pass_age_unit_required</th>\n",
       "      <th>presence_pass_dir_dec_requiredIfGroup</th>\n",
       "      <th>presence_pass_dir_tilt_correction_requiredIfGroup</th>\n",
       "      <th>presence_pass_geologic_classes_required</th>\n",
       "      <th>type_pass_lat_n_Number</th>\n",
       "      <th>value_pass_lat_s_checkMax</th>\n",
       "      <th>value_pass_lithologies_cv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tel Hazor</th>\n",
       "      <td>This study</td>\n",
       "      <td>hello</td>\n",
       "      <td>400</td>\n",
       "      <td>Tel Hazor</td>\n",
       "      <td>Archeological Site</td>\n",
       "      <td>35.568</td>\n",
       "      <td>35.568</td>\n",
       "      <td>5</td>\n",
       "      <td>Agate:Basalt</td>\n",
       "      <td>age column is required unless age_low is prese...</td>\n",
       "      <td>age_high column is required unless age is pres...</td>\n",
       "      <td>age_low column is required unless age is prese...</td>\n",
       "      <td>\"age_unit\" column is required</td>\n",
       "      <td>dir_dec column is required if column group Dir...</td>\n",
       "      <td>dir_tilt_correction column is required if colu...</td>\n",
       "      <td>\"geologic_classes\" column is required</td>\n",
       "      <td>\"hello\" should be a number</td>\n",
       "      <td>400.0 (lat_s) must be &lt;= 90.0 (90)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tel Megiddo</th>\n",
       "      <td>This study</td>\n",
       "      <td>32.585</td>\n",
       "      <td>32.585</td>\n",
       "      <td>Tel Megiddo</td>\n",
       "      <td>Archeological Site</td>\n",
       "      <td>35.185</td>\n",
       "      <td>35.185</td>\n",
       "      <td>5</td>\n",
       "      <td>Basalt:random</td>\n",
       "      <td>age column is required unless age_low is prese...</td>\n",
       "      <td>age_high column is required unless age is pres...</td>\n",
       "      <td>age_low column is required unless age is prese...</td>\n",
       "      <td>\"age_unit\" column is required</td>\n",
       "      <td>dir_dec column is required if column group Dir...</td>\n",
       "      <td>dir_tilt_correction column is required if colu...</td>\n",
       "      <td>\"geologic_classes\" column is required</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\"random\" is not in controlled vocabulary for l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              citations   lat_n   lat_s     location       location_type  \\\n",
       "location                                                                   \n",
       "Tel Hazor    This study   hello     400    Tel Hazor  Archeological Site   \n",
       "Tel Megiddo  This study  32.585  32.585  Tel Megiddo  Archeological Site   \n",
       "\n",
       "              lon_e   lon_w  dir_inc    lithologies  \\\n",
       "location                                              \n",
       "Tel Hazor    35.568  35.568        5   Agate:Basalt   \n",
       "Tel Megiddo  35.185  35.185        5  Basalt:random   \n",
       "\n",
       "                              presence_pass_age_requiredUnless  \\\n",
       "location                                                         \n",
       "Tel Hazor    age column is required unless age_low is prese...   \n",
       "Tel Megiddo  age column is required unless age_low is prese...   \n",
       "\n",
       "                         presence_pass_age_high_requiredUnless  \\\n",
       "location                                                         \n",
       "Tel Hazor    age_high column is required unless age is pres...   \n",
       "Tel Megiddo  age_high column is required unless age is pres...   \n",
       "\n",
       "                          presence_pass_age_low_requiredUnless  \\\n",
       "location                                                         \n",
       "Tel Hazor    age_low column is required unless age is prese...   \n",
       "Tel Megiddo  age_low column is required unless age is prese...   \n",
       "\n",
       "            presence_pass_age_unit_required  \\\n",
       "location                                      \n",
       "Tel Hazor     \"age_unit\" column is required   \n",
       "Tel Megiddo   \"age_unit\" column is required   \n",
       "\n",
       "                         presence_pass_dir_dec_requiredIfGroup  \\\n",
       "location                                                         \n",
       "Tel Hazor    dir_dec column is required if column group Dir...   \n",
       "Tel Megiddo  dir_dec column is required if column group Dir...   \n",
       "\n",
       "             presence_pass_dir_tilt_correction_requiredIfGroup  \\\n",
       "location                                                         \n",
       "Tel Hazor    dir_tilt_correction column is required if colu...   \n",
       "Tel Megiddo  dir_tilt_correction column is required if colu...   \n",
       "\n",
       "            presence_pass_geologic_classes_required  \\\n",
       "location                                              \n",
       "Tel Hazor     \"geologic_classes\" column is required   \n",
       "Tel Megiddo   \"geologic_classes\" column is required   \n",
       "\n",
       "                 type_pass_lat_n_Number           value_pass_lat_s_checkMax  \\\n",
       "location                                                                      \n",
       "Tel Hazor    \"hello\" should be a number  400.0 (lat_s) must be <= 90.0 (90)   \n",
       "Tel Megiddo                        None                                None   \n",
       "\n",
       "                                     value_pass_lithologies_cv  \n",
       "location                                                        \n",
       "Tel Hazor                                                 None  \n",
       "Tel Megiddo  \"random\" is not in controlled vocabulary for l...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate a DataFrame\n",
    "current_df = vu3.validate_df(loc_df, loc_dm, current_con)\n",
    "current_df.dropna(how='all', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'presence_pass_age_requiredUnless',\n",
       "       u'presence_pass_age_high_requiredUnless',\n",
       "       u'presence_pass_age_low_requiredUnless',\n",
       "       u'presence_pass_age_unit_required', u'type_pass_citations_List',\n",
       "       u'presence_pass_citations_requiredUnlessTable',\n",
       "       u'presence_pass_dir_dec_requiredIfGroup', u'type_pass_dir_inc_Number',\n",
       "       u'value_pass_dir_inc_checkMin', u'value_pass_dir_inc_checkMax'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get names of all validation column names added to dataframe by validate_df\n",
    "value_col_names, present_col_names, type_col_names, validation_col_names = vu3.get_validation_col_names(current_df)\n",
    "validation_col_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_pass_lat_n_Number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tel Hazor</th>\n",
       "      <td>\"hello\" should be a number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tel Megiddo</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 type_pass_lat_n_Number\n",
       "location                               \n",
       "Tel Hazor    \"hello\" should be a number\n",
       "Tel Megiddo                        None"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incorrect data type problems\n",
    "current_df[type_col_names].dropna(how='all', axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>presence_pass_age_requiredUnless</th>\n",
       "      <th>presence_pass_age_high_requiredUnless</th>\n",
       "      <th>presence_pass_age_low_requiredUnless</th>\n",
       "      <th>presence_pass_age_unit_required</th>\n",
       "      <th>presence_pass_dir_dec_requiredIfGroup</th>\n",
       "      <th>presence_pass_dir_tilt_correction_requiredIfGroup</th>\n",
       "      <th>presence_pass_geologic_classes_required</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tel Hazor</th>\n",
       "      <td>age column is required unless age_low is prese...</td>\n",
       "      <td>age_high column is required unless age is pres...</td>\n",
       "      <td>age_low column is required unless age is prese...</td>\n",
       "      <td>\"age_unit\" column is required</td>\n",
       "      <td>dir_dec column is required if column group Dir...</td>\n",
       "      <td>dir_tilt_correction column is required if colu...</td>\n",
       "      <td>\"geologic_classes\" column is required</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tel Megiddo</th>\n",
       "      <td>age column is required unless age_low is prese...</td>\n",
       "      <td>age_high column is required unless age is pres...</td>\n",
       "      <td>age_low column is required unless age is prese...</td>\n",
       "      <td>\"age_unit\" column is required</td>\n",
       "      <td>dir_dec column is required if column group Dir...</td>\n",
       "      <td>dir_tilt_correction column is required if colu...</td>\n",
       "      <td>\"geologic_classes\" column is required</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              presence_pass_age_requiredUnless  \\\n",
       "location                                                         \n",
       "Tel Hazor    age column is required unless age_low is prese...   \n",
       "Tel Megiddo  age column is required unless age_low is prese...   \n",
       "\n",
       "                         presence_pass_age_high_requiredUnless  \\\n",
       "location                                                         \n",
       "Tel Hazor    age_high column is required unless age is pres...   \n",
       "Tel Megiddo  age_high column is required unless age is pres...   \n",
       "\n",
       "                          presence_pass_age_low_requiredUnless  \\\n",
       "location                                                         \n",
       "Tel Hazor    age_low column is required unless age is prese...   \n",
       "Tel Megiddo  age_low column is required unless age is prese...   \n",
       "\n",
       "            presence_pass_age_unit_required  \\\n",
       "location                                      \n",
       "Tel Hazor     \"age_unit\" column is required   \n",
       "Tel Megiddo   \"age_unit\" column is required   \n",
       "\n",
       "                         presence_pass_dir_dec_requiredIfGroup  \\\n",
       "location                                                         \n",
       "Tel Hazor    dir_dec column is required if column group Dir...   \n",
       "Tel Megiddo  dir_dec column is required if column group Dir...   \n",
       "\n",
       "             presence_pass_dir_tilt_correction_requiredIfGroup  \\\n",
       "location                                                         \n",
       "Tel Hazor    dir_tilt_correction column is required if colu...   \n",
       "Tel Megiddo  dir_tilt_correction column is required if colu...   \n",
       "\n",
       "            presence_pass_geologic_classes_required  \n",
       "location                                             \n",
       "Tel Hazor     \"geologic_classes\" column is required  \n",
       "Tel Megiddo   \"geologic_classes\" column is required  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing column problems\n",
    "current_df[present_col_names].dropna(how='all', axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_pass_lat_s_checkMax</th>\n",
       "      <th>value_pass_lithologies_cv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tel Hazor</th>\n",
       "      <td>400.0 (lat_s) must be &lt;= 90.0 (90)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tel Megiddo</th>\n",
       "      <td>None</td>\n",
       "      <td>\"random\" is not in controlled vocabulary for l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      value_pass_lat_s_checkMax  \\\n",
       "location                                          \n",
       "Tel Hazor    400.0 (lat_s) must be <= 90.0 (90)   \n",
       "Tel Megiddo                                None   \n",
       "\n",
       "                                     value_pass_lithologies_cv  \n",
       "location                                                        \n",
       "Tel Hazor                                                 None  \n",
       "Tel Megiddo  \"random\" is not in controlled vocabulary for l...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value problems:\n",
    "current_df[value_col_names].dropna(how='all', axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>type_pass_lat_n_Number</th>\n",
       "      <th>value_pass_lat_s_checkMax</th>\n",
       "      <th>value_pass_lithologies_cv</th>\n",
       "      <th>issues</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tel Hazor</th>\n",
       "      <td>0</td>\n",
       "      <td>\"hello\" should be a number</td>\n",
       "      <td>400.0 (lat_s) must be &lt;= 90.0 (90)</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'value_pass_lat_s_checkMax': u'400.0 (lat_s)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tel Megiddo</th>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\"random\" is not in controlled vocabulary for l...</td>\n",
       "      <td>{u'value_pass_lithologies_cv': u'\"random\" is n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             num      type_pass_lat_n_Number  \\\n",
       "location                                       \n",
       "Tel Hazor      0  \"hello\" should be a number   \n",
       "Tel Megiddo    1                        None   \n",
       "\n",
       "                      value_pass_lat_s_checkMax  \\\n",
       "location                                          \n",
       "Tel Hazor    400.0 (lat_s) must be <= 90.0 (90)   \n",
       "Tel Megiddo                                None   \n",
       "\n",
       "                                     value_pass_lithologies_cv  \\\n",
       "location                                                         \n",
       "Tel Hazor                                                 None   \n",
       "Tel Megiddo  \"random\" is not in controlled vocabulary for l...   \n",
       "\n",
       "                                                        issues  \n",
       "location                                                        \n",
       "Tel Hazor    {u'value_pass_lat_s_checkMax': u'400.0 (lat_s)...  \n",
       "Tel Megiddo  {u'value_pass_lithologies_cv': u'\"random\" is n...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all row failures (can also print these failures to logfile OR stdout)\n",
    "failing_items = vu3.get_row_failures(current_df, value_col_names, type_col_names, verbose=False, outfile=None)\n",
    "failing_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-W- these rows have problems: [(0, 'Tel Hazor'), (1, 'Tel Megiddo')]\n",
      "-W- these columns contain bad values: lithologies, lat_n, lat_s\n",
      "-W- these required columns are missing: age_high, age_low, age, age_unit, dir_dec, dir_tilt_correction, geologic_classes\n",
      "bad rows: [(0, 'Tel Hazor'), (1, 'Tel Megiddo')]\n",
      "bad columns: [u'lat_n', u'lat_s', u'lithologies']\n",
      "missing columns: [u'age_high', u'age_low', u'age', u'age_unit', u'dir_dec', u'dir_tilt_correction', u'geologic_classes']\n"
     ]
    }
   ],
   "source": [
    "# get lists of: all rows with problems, all columns with problems, and all missing columns\n",
    "bad_rows, bad_cols, missing_cols = vu3.get_bad_rows_and_cols(current_df, validation_col_names, \n",
    "                                                        value_col_names, type_col_names, verbose=True)\n",
    "print \"bad rows:\", bad_rows\n",
    "print \"bad columns:\", bad_cols\n",
    "print \"missing columns:\", missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_bad_rows_and_cols(df, validation_names, type_col_names, value_col_names, verbose=False):\\n\\n    Input: validated DataFrame, all validation names, names of the type columns,\\n    names of the value columns, verbose (True or False).\\n    Output: list of rows with bad values, list of columns with bad values, \\n    list of missing (but required) columns.\\n\\n    df[\"num\"] = range(len(df))\\n    problems = df[validation_names.union([\"num\"])]\\n    all_problems = problems.dropna(how=\\'all\\', axis=0, subset=validation_names)\\n    value_problems = problems.dropna(how=\\'all\\', axis=0, subset=type_col_names.union(value_col_names))\\n    all_problems = all_problems.dropna(how=\\'all\\', axis=1)\\n    value_problems = value_problems.dropna(how=\\'all\\', axis=1)\\n    if not len(problems):\\n        return None, None, None\\n    #\\n    bad_cols = all_problems.columns\\n    prefixes = [\"value_pass_\", \"type_pass_\"]\\n    missing_prefix = \"presence_pass_\"\\n    problem_cols = []\\n    missing_cols = []\\n    long_missing_cols = []\\n    problem_rows = []\\n    for col in bad_cols:\\n        pre, stripped_col = vu3.extract_col_name(col)\\n        for prefix in prefixes:\\n            if col.startswith(prefix):\\n                problem_cols.append(stripped_col)\\n                continue\\n        if col.startswith(missing_prefix):\\n            missing_cols.append(stripped_col)\\n            long_missing_cols.append(col)\\n            \\n    if len(value_problems):\\n        bad_rows = zip(list(value_problems[\"num\"]), list(value_problems.index))\\n    else:\\n        bad_rows = []\\n    if verbose:\\n        if bad_rows:\\n            if len(bad_rows) > 20:\\n                print \"-W- these rows have problems:\", bad_rows[:20], \" ...\",\\n                print \"(for full error output see error file)\"\\n            else:\\n                print \"-W- these rows have problems:\", bad_rows\\n        if problem_cols:\\n            print \"-W- these columns contain bad values:\", \", \".join(set(problem_cols))\\n        if missing_cols:\\n            print \"-W- these required columns are missing:\", \", \".join(missing_cols)\\n    return bad_rows, problem_cols, missing_cols\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def get_bad_rows_and_cols(df, validation_names, type_col_names, value_col_names, verbose=False):\n",
    "\n",
    "    Input: validated DataFrame, all validation names, names of the type columns,\n",
    "    names of the value columns, verbose (True or False).\n",
    "    Output: list of rows with bad values, list of columns with bad values, \n",
    "    list of missing (but required) columns.\n",
    "\n",
    "    df[\"num\"] = range(len(df))\n",
    "    problems = df[validation_names.union([\"num\"])]\n",
    "    all_problems = problems.dropna(how='all', axis=0, subset=validation_names)\n",
    "    value_problems = problems.dropna(how='all', axis=0, subset=type_col_names.union(value_col_names))\n",
    "    all_problems = all_problems.dropna(how='all', axis=1)\n",
    "    value_problems = value_problems.dropna(how='all', axis=1)\n",
    "    if not len(problems):\n",
    "        return None, None, None\n",
    "    #\n",
    "    bad_cols = all_problems.columns\n",
    "    prefixes = [\"value_pass_\", \"type_pass_\"]\n",
    "    missing_prefix = \"presence_pass_\"\n",
    "    problem_cols = []\n",
    "    missing_cols = []\n",
    "    long_missing_cols = []\n",
    "    problem_rows = []\n",
    "    for col in bad_cols:\n",
    "        pre, stripped_col = vu3.extract_col_name(col)\n",
    "        for prefix in prefixes:\n",
    "            if col.startswith(prefix):\n",
    "                problem_cols.append(stripped_col)\n",
    "                continue\n",
    "        if col.startswith(missing_prefix):\n",
    "            missing_cols.append(stripped_col)\n",
    "            long_missing_cols.append(col)\n",
    "            \n",
    "    if len(value_problems):\n",
    "        bad_rows = zip(list(value_problems[\"num\"]), list(value_problems.index))\n",
    "    else:\n",
    "        bad_rows = []\n",
    "    if verbose:\n",
    "        if bad_rows:\n",
    "            if len(bad_rows) > 20:\n",
    "                print \"-W- these rows have problems:\", bad_rows[:20], \" ...\",\n",
    "                print \"(for full error output see error file)\"\n",
    "            else:\n",
    "                print \"-W- these rows have problems:\", bad_rows\n",
    "        if problem_cols:\n",
    "            print \"-W- these columns contain bad values:\", \", \".join(set(problem_cols))\n",
    "        if missing_cols:\n",
    "            print \"-W- these required columns are missing:\", \", \".join(missing_cols)\n",
    "    return bad_rows, problem_cols, missing_cols\n",
    "\"\"\"  \n",
    "#a, b, c = get_bad_rows_and_cols(current_df, validation_col_names)\n",
    "#if a:\n",
    "#    print \"bad rows:\", a[:10]\n",
    "#    print \"problems:\", b[:10]\n",
    "#    print \"missing:\", c[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Validating sites\n",
      "hz07\t2\n",
      "type\tdir_tilt_correction\t\"not a number\" should be a number\n",
      "mgh12t1\t18\n",
      "value\tcriteria\tThis value: \"ACCEPT\" is not found in: criteria.criterion\n",
      "mgh12t1\t19\n",
      "value\tcriteria\tThis value: \"ACCEPT\" is not found in: criteria.criterion\n",
      "mgj06\t21\n",
      "value\tage_high\t-2800 (age_high) must be >= -2600 (age_low)\n",
      "value\tage_low\t-2600 (age_low) must be <= -2800 (age_high)\n",
      "mgk06\t22\n",
      "value\tage_high\t-1180 (age_high) must be >= -1130 (age_low)\n",
      "value\tage_low\t-1130 (age_low) must be <= -1180 (age_high)\n",
      "mgk09t1\t26\n",
      "value\tcriteria\tThis value: \"ACCEPT\" is not found in: criteria.criterion\n",
      "mgk09t1\t27\n",
      "value\tcriteria\tThis value: \"ACCEPT\" is not found in: criteria.criterion\n",
      "mgq04t1\t30\n",
      "value\tcriteria\tThis value: \"ACCEPT\" is not found in: criteria.criterion\n",
      "mgq04t1\t31\n",
      "value\tcriteria\tThis value: \"ACCEPT\" is not found in: criteria.criterion\n",
      "mgq05t1\t34\n",
      "value\tcriteria\tThis value: \"ACCEPT\" is not found in: criteria.criterion\n",
      "mgq05t1\t35\n",
      "value\tcriteria\tThis value: \"ACCEPT\" is not found in: criteria.criterion\n",
      "mgq05t2\t37\n",
      "value\tcriteria\tThis value: \"ACCEPT\" is not found in: criteria.criterion\n",
      "mgq05t2\t38\n",
      "value\tcriteria\tThis value: \"ACCEPT\" is not found in: criteria.criterion\n",
      "-W- these rows have problems: [(2, 'hz07'), (18, 'mgh12t1'), (19, 'mgh12t1'), (21, 'mgj06'), (22, 'mgk06'), (26, 'mgk09t1'), (27, 'mgk09t1'), (30, 'mgq04t1'), (31, 'mgq04t1'), (34, 'mgq05t1'), (35, 'mgq05t1'), (37, 'mgq05t2'), (38, 'mgq05t2')]\n",
      "-W- these columns contain bad values: age_high, dir_tilt_correction, age_low, criteria\n",
      "-W- these required columns are missing: age_high, age_low, result_quality\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/sites_errors.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sites'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#dir_path = os.path.join(os.getcwd(), '3_0', 'Megiddo')\n",
    "#con = Contribution(dir_path)\n",
    "the_con = con\n",
    "\n",
    "# validate table calls ALL of the above functions\n",
    "vu3.validate_table(the_con, 'sites', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating measurements\n",
      "-I- Validating measurements\n",
      "-W- these rows have problems: [(1, 'mgh05a01:LP-PI-TRM1')]\n",
      "-W- these columns contain bad values: specimen, magn_moment\n",
      "-W- these required columns are missing: experiment, instrument_codes\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/measurements_errors.txt\n",
      "--\n",
      "validating ages\n",
      "-I- Validating ages\n",
      "-W- these rows have problems: [(1, 1), (19, 19), (20, 20)]\n",
      "-W- these columns contain bad values: age_high, age, age_low, site\n",
      "-W- these required columns are missing: citations\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/ages_errors.txt\n",
      "--\n",
      "validating sites\n",
      "-I- Validating sites\n",
      "-W- these rows have problems: [(2, 'hz07'), (18, 'mgh12t1'), (19, 'mgh12t1'), (21, 'mgj06'), (22, 'mgk06'), (26, 'mgk09t1'), (27, 'mgk09t1'), (30, 'mgq04t1'), (31, 'mgq04t1'), (34, 'mgq05t1'), (35, 'mgq05t1'), (37, 'mgq05t2'), (38, 'mgq05t2')]\n",
      "-W- these columns contain bad values: age_high, dir_tilt_correction, age_low, criteria\n",
      "-W- these required columns are missing: age_high, age_low, result_quality\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/sites_errors.txt\n",
      "--\n",
      "validating locations\n",
      "-I- Validating locations\n",
      "-W- these rows have problems: [(0, 'Tel Hazor'), (1, 'Tel Megiddo')]\n",
      "-W- these columns contain bad values: lithologies, lat_n, lat_s\n",
      "-W- these required columns are missing: age_high, age_low, age, age_unit, dir_dec, dir_tilt_correction, geologic_classes\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/locations_errors.txt\n",
      "--\n",
      "validating samples\n",
      "-I- Validating samples\n",
      "-W- these rows have problems: [(0, 'hz05a'), (1, 'hz05a'), (2, 'hz05b'), (3, 'hz05b'), (5, 'hz05c'), (7, 'hz05e'), (9, 'hz05f'), (11, 'hz05g'), (14, 'hz06a'), (16, 'hz06b'), (18, 'hz06c'), (20, 'hz07a'), (22, 'hz07b'), (24, 'hz07c'), (26, 'hz07d'), (28, 'hz07e'), (30, 'hz09a'), (34, 'hz10b'), (36, 'hz10c'), (38, 'hz10d')]  ... (for full error output see error file)\n",
      "-W- these columns contain bad values: lon, site, criteria, lat, cooling_rate, specimens\n",
      "-W- these required columns are missing: citations, result_quality\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/samples_errors.txt\n",
      "--\n",
      "validating criteria\n",
      "-I- Validating criteria\n",
      "-I- No row errors found!\n",
      "--\n",
      "validating contribution\n",
      "-I- Validating contribution\n",
      "-I- No row errors found!\n",
      "--\n",
      "validating specimens\n",
      "-I- Validating specimens\n",
      "-W- these rows have problems: [(0, 'hz05a1'), (1, 'hz05a1'), (2, 'hz05a1'), (3, 'hz05a2'), (4, 'hz05a2'), (5, 'hz05a2'), (6, 'hz05a3'), (7, 'hz05a3'), (8, 'hz05a3'), (9, 'hz05b1'), (10, 'hz05b1'), (11, 'hz05b1'), (12, 'hz05b2'), (13, 'hz05b2'), (14, 'hz05b2'), (15, 'hz05b3'), (16, 'hz05b3'), (17, 'hz05b3'), (18, 'hz05b4'), (19, 'hz05b4')]  ... (for full error output see error file)\n",
      "-W- these columns contain bad values: experiments\n",
      "-W- these required columns are missing: aniso_tilt_correction, result_type\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/specimens_errors.txt\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "## run through and validate entire contribution (call validate_table on each table)\n",
    "vu3.validate_contribution(the_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Removing:  ['citation_label', 'compilation', 'calculation_type', 'average_n_lines', 'average_n_planes', 'specimen_grade', 'site_vgp_lat', 'site_vgp_lon', 'direction_type', 'specimen_Z', 'magic_instrument_codes', 'cooling_rate_corr', 'cooling_rate_mcd', 'anisotropy_atrm_alt', 'anisotropy_apar_perc', 'anisotropy_F', 'anisotropy_F_crit', 'specimen_scat', 'specimen_gmax', 'specimen_frac', 'site_vadm', 'site_lon', 'site_vdm', 'site_lat', 'measurement_chi', 'specimen_k_prime', 'specimen_k_prime_sse', 'external_database_names', 'external_database_ids', 'Further Notes', 'Typology', 'Notes (Year/Area/Locus/Level)', 'Site', 'Object Number']\n",
      "-\n",
      "-I- locations file successfully read in\n",
      "-I- Validating locations\n",
      "-W- these rows have problems: [(0, 'McMurdo'), (1, 'McMurdo'), (2, 'McMurdo'), (3, 'McMurdo'), (4, 'McMurdo')]\n",
      "-W- these columns contain bad values: lat_n, lon_w, pole_lon, criteria, lon_e, dir_dec, lat_s\n",
      "-W- these required columns are missing: age, dir_tilt_correction, geologic_classes, lithologies\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/locations_errors.txt\n",
      "-I- appending locations data to /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-I- locations written to  /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-\n",
      "-I- samples file successfully read in\n",
      "-I- Validating samples\n",
      "-W- these rows have problems: [(752, 'mc157d')]\n",
      "-W- these columns contain bad values: azimuth\n",
      "-W- these required columns are missing: result_quality, result_type\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/samples_errors.txt\n",
      "-I- appending samples data to /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-I- samples written to  /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-\n",
      "-I- specimens file successfully read in\n",
      "-I- Validating specimens\n",
      "-W- these rows have problems: [(0, 'mc01a'), (1, 'mc01a'), (2, 'mc01b'), (3, 'mc01b'), (4, 'mc01c'), (5, 'mc01d'), (6, 'mc01d'), (7, 'mc01e'), (8, 'mc01f'), (9, 'mc01g'), (10, 'mc01g'), (11, 'mc01h'), (12, 'mc02a'), (13, 'mc02b'), (14, 'mc02c'), (15, 'mc02e'), (16, 'mc02f'), (17, 'mc02f'), (18, 'mc02g'), (19, 'mc02h')]  ... (for full error output see error file)\n",
      "-W- these columns contain bad values: int_b_beta, int_md, dir_dec\n",
      "-W- these required columns are missing: result_quality, result_type\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/specimens_errors.txt\n",
      "-I- appending specimens data to /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-I- specimens written to  /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-\n",
      "-I- sites file successfully read in\n",
      "-I- Validating sites\n",
      "-W- these rows have problems: [(0, 'mc01'), (1, 'mc01'), (2, 'mc01'), (3, 'mc01'), (4, 'mc02'), (5, 'mc02'), (6, 'mc02'), (7, 'mc02'), (8, 'mc03'), (9, 'mc03'), (10, 'mc03'), (11, 'mc03'), (12, 'mc04'), (13, 'mc04'), (14, 'mc04'), (15, 'mc04'), (16, 'mc06'), (17, 'mc06'), (18, 'mc06'), (19, 'mc07')]  ... (for full error output see error file)\n",
      "-W- these columns contain bad values: vgp_lon, dir_dec, lon, criteria\n",
      "-W- these required columns are missing: age, result_quality\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/sites_errors.txt\n",
      "-I- appending sites data to /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-I- sites written to  /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-\n",
      "-I- ages file successfully read in\n",
      "-I- Validating ages\n",
      "-I- No row errors found!\n",
      "-I- appending ages data to /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-I- ages written to  /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-\n",
      "-I- measurements file successfully read in\n",
      "-I- Validating measurements\n",
      "-W- these rows have problems: [(21129, 'mc04c-1-LP-HYS1'), (21130, 'mc04c-1-LP-HYS2'), (21131, 'mc04c-1-LP-HYS3'), (21132, 'mc04c-1-LP-HYS4'), (21133, 'mc04c-1-LP-HYS5'), (21134, 'mc04c-1-LP-HYS6'), (21135, 'mc04c-1-LP-HYS7'), (21136, 'mc04c-1-LP-HYS8'), (21137, 'mc04c-1-LP-HYS9'), (21138, 'mc04c-1-LP-HYS10'), (21139, 'mc04c-1-LP-HYS11'), (21140, 'mc04c-1-LP-HYS12'), (21141, 'mc04c-1-LP-HYS13'), (21142, 'mc04c-1-LP-HYS14'), (21143, 'mc04c-1-LP-HYS15'), (21144, 'mc04c-1-LP-HYS16'), (21145, 'mc04c-1-LP-HYS17'), (21146, 'mc04c-1-LP-HYS18'), (21147, 'mc04c-1-LP-HYS19'), (21148, 'mc04c-1-LP-HYS20')]  ... (for full error output see error file)\n",
      "-W- these columns contain bad values: dir_dec\n",
      "-W- these required columns are missing: instrument_codes, quality\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/measurements_errors.txt\n",
      "-I- appending measurements data to /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-I- measurements written to  /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-\n",
      "-I- criteria file successfully read in\n",
      "-I- Validating criteria\n",
      "-I- No row errors found!\n",
      "-I- appending criteria data to /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-I- criteria written to  /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-\n",
      "-I- contribution file successfully read in\n",
      "-I- Validating contribution\n",
      "-I- No row errors found!\n",
      "-I- appending contribution data to /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-I- contribution written to  /Users/nebula/Python/PmagPy/3_0/McMurdo/upload.txt\n",
      "-\n",
      "images is bad or non-existent - skipping \n",
      "Finished preparing upload file: /Users/nebula/Python/PmagPy/3_0/McMurdo/McMurdo_29.Jul.2016.txt \n",
      "-W- validation of upload file has failed.\n",
      "These tables have errors: locations, samples, specimens, sites, measurements\n",
      "Please fix above errors and try again.\n",
      "You may run into problems if you try to upload this file to the MagIC database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " 'file validation has failed.  You may run into problems if you try to upload this file.',\n",
       " ['locations', 'samples', 'specimens', 'sites', 'measurements'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.0 upload function, including full validation\n",
    "\n",
    "import time\n",
    "from pmagpy import pmag\n",
    "reload(pmag)\n",
    "from pmagpy import data_model3 as data_model\n",
    "reload(nb)\n",
    "\n",
    "\n",
    "def upload_magic3(concat=0, dir_path='.', dmodel=None):\n",
    "    \"\"\"                                                                                                                              \n",
    "    Finds all magic files in a given directory, and compiles them into an upload.txt file which can be uploaded into the MagIC datab\\\n",
    "ase.                                                                                                                                 \n",
    "    returns a tuple of either: (False, error_message, errors) if there was a problem creating/validating the upload file             \n",
    "    or: (filename, '', None) if the upload was fully successful                                                                      \n",
    "    \"\"\"\n",
    "    SpecDone=[]\n",
    "    locations = []\n",
    "    concat = int(concat)\n",
    "    dtypes = [\"locations\", \"samples\", \"specimens\", \"sites\", \"ages\", \"measurements\",\n",
    "                  \"criteria\", \"contribution\", \"images\"]\n",
    "    file_names = [os.path.join(dir_path, dtype + \".txt\") for dtype in dtypes]\n",
    "    con = Contribution(dir_path)\n",
    "    # begin the upload process                                                                                                       \n",
    "    up = os.path.join(dir_path, \"upload.txt\")\n",
    "    if os.path.exists(up):\n",
    "        os.remove(up)\n",
    "    RmKeys = ['citation_label', 'compilation', 'calculation_type', 'average_n_lines', 'average_n_planes',\n",
    "              'specimen_grade', 'site_vgp_lat', 'site_vgp_lon', 'direction_type', 'specimen_Z',\n",
    "              'magic_instrument_codes', 'cooling_rate_corr', 'cooling_rate_mcd', 'anisotropy_atrm_alt',\n",
    "              'anisotropy_apar_perc', 'anisotropy_F', 'anisotropy_F_crit', 'specimen_scat',\n",
    "              'specimen_gmax','specimen_frac', 'site_vadm', 'site_lon', 'site_vdm', 'site_lat',\n",
    "              'measurement_chi', 'specimen_k_prime','specimen_k_prime_sse','external_database_names',\n",
    "              'external_database_ids', 'Further Notes', 'Typology', 'Notes (Year/Area/Locus/Level)',\n",
    "              'Site', 'Object Number']\n",
    "    print \"-I- Removing: \", RmKeys\n",
    "    CheckDec = ['_dec', '_lon', '_azimuth', 'dip_direction']\n",
    "    CheckSign = ['specimen_b_beta']\n",
    "    last = file_names[-1]\n",
    "    methods, first_file = [], 1\n",
    "    failing = []\n",
    "    if not dmodel:\n",
    "        dmodel = data_model.DataModel()\n",
    "    for file_type in dtypes:\n",
    "        print \"-\"\n",
    "    # read in the data\n",
    "        #Data, file_type = pmag.magic_read(File)\n",
    "        if file_type not in con.tables.keys():\n",
    "            print \"-I- No {} file found, continuing\".format(file_type)\n",
    "            continue\n",
    "        container = con.tables[file_type]\n",
    "        df = container.df\n",
    "        if len(df):\n",
    "            print \"-I- {} file successfully read in\".format(file_type)\n",
    "    # make some adjustments to clean up data\n",
    "            # drop non MagIC keys\n",
    "            DropKeys = set(RmKeys).intersection(df.columns)\n",
    "            df.drop(DropKeys, axis=1, inplace=True)\n",
    "            # make sure int_b_beta is positive\n",
    "            if 'int_b_beta' in df.columns:\n",
    "                df['int_b_beta'] = df['int_b_beta'].astype(float).apply(abs)\n",
    "            # make all declinations/azimuths/longitudes in range 0=>360.\n",
    "            relevant_cols = vu3.get_degree_cols(df)\n",
    "            for col in relevant_cols:\n",
    "                df[col] = df[col].apply(pmag.adjust_val_to_360)\n",
    "            # get list of location names\n",
    "            if file_type == 'locations':\n",
    "                locations = sorted(df['location'].unique())              \n",
    "            ## LJ: leave this for validations??\n",
    "            # use only highest priority orientation -- not sure how this works\n",
    "            elif file_type == 'samples':\n",
    "                #orient,az_type=pmag.get_orient(Data,rec['sample'])\n",
    "                pass\n",
    "            # include only specimen records with samples\n",
    "            elif file_type == 'specimens':\n",
    "                df = df[df['sample'].notnull()]\n",
    "                if 'samples' in con.tables:\n",
    "                    samp_df = con.tables['samples'].df\n",
    "                    df = df[df['sample'].isin(samp_df.index.unique())]\n",
    "            # include only measurements with specmiens\n",
    "            elif file_type == 'measurements':\n",
    "                df = df[df['specimen'].notnull()]\n",
    "                if 'specimens' in con.tables:\n",
    "                    spec_df = con.tables['specimens'].df\n",
    "                    df = df[df['specimen'].isin(spec_df.index.unique())]               \n",
    "    # run validations\n",
    "            res = vu3.validate_table(con, file_type)#, verbose=True)\n",
    "            if res:\n",
    "                failing.append(res)\n",
    "    # write out the data\n",
    "            if len(df):\n",
    "                container.write_magic_file(up, append=True)\n",
    "    # write out the file separator                                                                                                   \n",
    "            f = open(up, 'a')\n",
    "            f.write('>>>>>>>>>>\\n')\n",
    "            f.close()\n",
    "            print \"-I-\", file_type, 'written to ',up\n",
    "        else:\n",
    "            #print 'File:', File\n",
    "            print file_type, 'is bad or non-existent - skipping '\n",
    "    ## add to existing file\n",
    "    if concat == 1:\n",
    "        f = open(up, 'a')\n",
    "        f.write('>>>>>>>>>>\\n')\n",
    "        f.close()\n",
    "     \n",
    "    if not os.path.isfile(up):\n",
    "        print \"no data found, upload file not created\"\n",
    "        return False, \"no data found, upload file not created\", None\n",
    "\n",
    "    #rename upload.txt according to location + timestamp                                                                             \n",
    "    format_string = \"%d.%b.%Y\"\n",
    "    if locations:\n",
    "        locs = set(locations)\n",
    "        locs = sorted(locs)[:3]\n",
    "        #location = locations[0].replace(' ', '_')\n",
    "        locs = [loc.replace(' ', '-') for loc in locs]\n",
    "        location = \"_\".join(locs)\n",
    "        new_up = location + '_' + time.strftime(format_string) + '.txt'\n",
    "    else:\n",
    "        new_up = 'unknown_location_' + time.strftime(format_string) + '.txt'\n",
    "\n",
    "    new_up = os.path.join(dir_path, new_up)\n",
    "    if os.path.isfile(new_up):\n",
    "        fname, extension = os.path.splitext(new_up)\n",
    "        for i in range(1, 100):\n",
    "            if os.path.isfile(fname + \"_\" + str(i) + extension):\n",
    "                continue\n",
    "            else:\n",
    "                new_up = fname + \"_\" + str(i) + extension\n",
    "                break\n",
    "    if not up:\n",
    "        print \"-W- Could not create an upload file\"\n",
    "        return\n",
    "    os.rename(up, new_up)\n",
    "    print \"Finished preparing upload file: {} \".format(new_up)\n",
    "    if failing:\n",
    "        print \"-W- validation of upload file has failed.\"\n",
    "        print \"These tables have errors: {}\".format(\", \".join(failing))\n",
    "        print \"Please fix above errors and try again.\"\n",
    "        print \"You may run into problems if you try to upload this file to the MagIC database.\"\n",
    "        return False, \"file validation has failed.  You may run into problems if you try to upload this file.\", failing\n",
    "    else:\n",
    "        print \"-I- Your file has passed validation.  Yay!\"\n",
    "    return new_up, '', None\n",
    "\n",
    "\n",
    "#dir_path = os.path.join(os.getcwd(), '3_0', 'Megiddo')\n",
    "#dir_path = os.path.join(os.getcwd(), '3_0', 'Osler')\n",
    "dir_path = os.path.join(os.getcwd(), '3_0', 'McMurdo')\n",
    "\n",
    "upload_magic3(dir_path=dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in an existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>two</th>\n",
       "      <th>three</th>\n",
       "      <th>four</th>\n",
       "      <th>five</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   one  two  three  four  five\n",
       "0    9  NaN    8.0     4     6\n",
       "1    4  9.0    4.0     9     7\n",
       "2    8  7.0    NaN     8     9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep all of df1, add in any extra from df2\n",
    "df1 = pd.DataFrame(np.random.randint(1, 10, (3, 5)), columns=['one', 'two', 'three', 'four', 'five'])\n",
    "df1.iloc[0, 1] = np.nan\n",
    "df1.iloc[2, 2] = np.nan\n",
    "df2 = pd.DataFrame(np.random.randint(1, 10, (3, 5)), columns=['one', 'three', 'five', 'seven', 'nine'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>three</th>\n",
       "      <th>five</th>\n",
       "      <th>seven</th>\n",
       "      <th>nine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   one  three  five  seven  nine\n",
       "0    9      3     7      5     7\n",
       "1    8      2     9      3     2\n",
       "2    1      3     5      6     5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>two</th>\n",
       "      <th>three</th>\n",
       "      <th>four</th>\n",
       "      <th>five</th>\n",
       "      <th>nine</th>\n",
       "      <th>seven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   one  two  three  four  five  nine  seven\n",
       "0    9  NaN    8.0     4     6     7      5\n",
       "1    4  9.0    4.0     9     7     2      3\n",
       "2    8  7.0    3.0     8     9     5      6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_df2_cols = df2.columns.difference(df1.columns)\n",
    "unique_df2 = df2[unique_df2_cols]\n",
    "\n",
    "# this adds in all the unique columns that weren't in df1\n",
    "concat_df = pd.concat([df1, unique_df2], axis=1)\n",
    "# fills in null values in df1 with values from df2\n",
    "concat_df.fillna(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! rm *_errors.txt\n",
    "! rm ./3_0/McMurdo/McMurdo*.txt\n",
    "#! rm ./3_0/Megiddo/Tel-Hazor*.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
