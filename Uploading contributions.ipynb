{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nebula/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/__init__.py:1350: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got full_df\n",
      "got parsed_df\n",
      "got full_df\n",
      "got parsed_df\n"
     ]
    }
   ],
   "source": [
    "# do basic imports and unpack McMurdo data\n",
    "\n",
    "#from pmagpy import ipmag\n",
    "#reload(ipmag)\n",
    "from pmagpy import pmag\n",
    "from programs import new_builder as nb\n",
    "from programs import data_model3\n",
    "reload(data_model3)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from programs.new_builder import Contribution\n",
    "\n",
    "import pmagpy.controlled_vocabularies3 as cv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-W- No such file: /Users/nebula/Python/PmagPy/3_0/Megiddo/images.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_high</th>\n",
       "      <th>age_low</th>\n",
       "      <th>age_unit</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>method_codes</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-740</td>\n",
       "      <td>-732</td>\n",
       "      <td>-800</td>\n",
       "      <td>Years Cal AD (+/-)</td>\n",
       "      <td>\"Tel-Hazor chronology. 2015 revision. Amnon Be...</td>\n",
       "      <td>Tel Hazor</td>\n",
       "      <td>GM-C14:GM-CC-ARCH</td>\n",
       "      <td>hz05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a string</td>\n",
       "      <td>-732</td>\n",
       "      <td>1e+12</td>\n",
       "      <td>Years Cal AD (+/-)</td>\n",
       "      <td>\"Tel-Hazor chronology. 2015 revision. Amnon Be...</td>\n",
       "      <td>Tel Hazor</td>\n",
       "      <td>GM-C14:GM-CC-ARCH</td>\n",
       "      <td>fake site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-850</td>\n",
       "      <td>-800</td>\n",
       "      <td>-900</td>\n",
       "      <td>Years Cal AD (+/-)</td>\n",
       "      <td>\"Tel-Hazor chronology. 2015 revision. Amnon Be...</td>\n",
       "      <td>Tel Hazor</td>\n",
       "      <td>GM-C14:GM-CC-ARCH</td>\n",
       "      <td>hz07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-950</td>\n",
       "      <td>-900</td>\n",
       "      <td>-1000</td>\n",
       "      <td>Years Cal AD (+/-)</td>\n",
       "      <td>\"Tel-Hazor chronology. 2015 revision. Amnon Be...</td>\n",
       "      <td>Tel Hazor</td>\n",
       "      <td>GM-C14:GM-CC-ARCH</td>\n",
       "      <td>hz09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-950</td>\n",
       "      <td>-900</td>\n",
       "      <td>-1000</td>\n",
       "      <td>Years Cal AD (+/-)</td>\n",
       "      <td>\"Tel-Hazor chronology. 2015 revision. Amnon Be...</td>\n",
       "      <td>Tel Hazor</td>\n",
       "      <td>GM-C14:GM-CC-ARCH</td>\n",
       "      <td>hz10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age age_high age_low            age_unit  \\\n",
       "0      -740     -732    -800  Years Cal AD (+/-)   \n",
       "1  a string     -732   1e+12  Years Cal AD (+/-)   \n",
       "2      -850     -800    -900  Years Cal AD (+/-)   \n",
       "3      -950     -900   -1000  Years Cal AD (+/-)   \n",
       "4      -950     -900   -1000  Years Cal AD (+/-)   \n",
       "\n",
       "                                         description   location  \\\n",
       "0  \"Tel-Hazor chronology. 2015 revision. Amnon Be...  Tel Hazor   \n",
       "1  \"Tel-Hazor chronology. 2015 revision. Amnon Be...  Tel Hazor   \n",
       "2  \"Tel-Hazor chronology. 2015 revision. Amnon Be...  Tel Hazor   \n",
       "3  \"Tel-Hazor chronology. 2015 revision. Amnon Be...  Tel Hazor   \n",
       "4  \"Tel-Hazor chronology. 2015 revision. Amnon Be...  Tel Hazor   \n",
       "\n",
       "        method_codes       site  \n",
       "0  GM-C14:GM-CC-ARCH       hz05  \n",
       "1  GM-C14:GM-CC-ARCH  fake site  \n",
       "2  GM-C14:GM-CC-ARCH       hz07  \n",
       "3  GM-C14:GM-CC-ARCH       hz09  \n",
       "4  GM-C14:GM-CC-ARCH       hz10  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = os.path.join(os.getcwd(), '3_0', 'Megiddo')\n",
    "con = Contribution(dir_path)\n",
    "\n",
    "loc_dm = con.tables['locations'].data_model.dm['locations']\n",
    "loc_df = con.tables['locations'].df\n",
    "site_dm = con.tables['sites'].data_model.dm['sites']\n",
    "site_df = con.tables['sites'].df\n",
    "samp_df = con.tables['samples'].df\n",
    "samp_dm = con.tables['samples'].data_model.dm['samples']\n",
    "spec_df = con.tables['specimens'].df\n",
    "spec_dm = con.tables['specimens'].data_model.dm['specimens']\n",
    "age_df = con.tables['ages'].df\n",
    "age_dm = con.tables['ages'].data_model.dm['ages']\n",
    "meas_df = con.tables['measurements'].df\n",
    "meas_dm = con.tables['measurements'].data_model.dm['measurements']\n",
    "cont_df = con.tables['contribution'].df\n",
    "cont_dm = con.tables['contribution'].data_model.dm['contribution']\n",
    "crit_df = con.tables['criteria'].df\n",
    "crit_dm = con.tables['criteria'].data_model.dm['criteria']\n",
    "\n",
    "\n",
    "current_con = con\n",
    "\n",
    "# mess up some validations for locations\n",
    "loc_df.loc['Tel Hazor', 'lat_s'] = 400.\n",
    "loc_df['dir_inc'] = 5\n",
    "loc_df.loc['Tel Hazor', 'lat_n'] = 'hello'\n",
    "loc_df.loc[:, 'lithologies'] = [\"Agate:Basalt\", \"Basalt:random\"]\n",
    "#current_con.tables.pop('sites')\n",
    "\n",
    "# mess up some validations for sites\n",
    "site_df.pop('age')\n",
    "site_df['dir_tilt_correction'] = 1\n",
    "site_df['dir_tilt_correction'] = 'not a number'\n",
    "site_df.iloc[0, list(site_df.columns).index('lithologies')] = \"Angrite:Basalt\"\n",
    "site_df.iloc[1, list(site_df.columns).index('lithologies')] = \"angrite : basalt\"\n",
    "\n",
    "# mess up some validations for ages\n",
    "age_df.ix[1]['age'] = 'a string'\n",
    "age_df.ix[1]['site'] = 'fake site'\n",
    "age_df.ix[1]['age_low'] = 1000000000000.\n",
    "age_df.pop('citations')\n",
    "\n",
    "# mess up some validations for samples\n",
    "samp_df.pop('citations')\n",
    "samp_df.iloc[0].lon = 600.\n",
    "samp_df.iloc[0].age = \"another string\"\n",
    "samp_df.iloc[0].lat = \"stringy\"\n",
    "samp_df.iloc[1].lat = 'hello'\n",
    "samp_df.iloc[2].specimens = \"hz05a2:fake\"\n",
    "samp_df.iloc[3].specimens = \"fake : hz05a1\"\n",
    "samp_df.iloc[5].specimens = 'fake_specimen'\n",
    "samp_df.iloc[7].site = 'fake_site'\n",
    "samp_df.iloc[0].cooling_rate = 'a string'\n",
    "\n",
    "# mess up some validations for measurements\n",
    "meas_df.loc['mgh05a01:LP-PI-TRM1', 'magn_moment'] = 2\n",
    "meas_df.loc['mgh05a01:LP-PI-TRM1', 'specimen'] = \"fake_specimen\"\n",
    "meas_df.pop('experiment')\n",
    "\n",
    "#current_df.head()\n",
    "#current_df.head()\n",
    "age_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Importing controlled vocabularies from https://earthref.org\n"
     ]
    }
   ],
   "source": [
    "import pmagpy.controlled_vocabularies3 as cv\n",
    "reload(cv)\n",
    "vocab = cv.Vocabulary()\n",
    "vocabulary, possible_vocabulary = vocab.get_controlled_vocabularies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## validation functions\n",
    "\n",
    "\n",
    "# need to add requiredOneInGroup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def requiredUnless(col_name, arg, dm, df):\n",
    "    \"\"\"\n",
    "    Arg is a string in the format \"str1, str2, ...\"\n",
    "    Each string will be a column name.\n",
    "    Col_name is required in df unless each column from arg is present.\n",
    "    \"\"\"\n",
    "    arg_list = arg.split(\",\")\n",
    "    arg_list = [arg.strip('\"') for arg in arg_list]\n",
    "    msg = \"\"\n",
    "    for a in arg_list:\n",
    "        # ignore validations that reference a different table\n",
    "        if \".\" in a:\n",
    "            continue\n",
    "        if a not in df.columns:\n",
    "            msg += \"{} column is required unless {} is present.  \".format(col_name, a)\n",
    "    if msg:\n",
    "        return msg\n",
    "    else:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def requiredUnlessTable(col_name, arg, dm, df):\n",
    "    \"\"\"\n",
    "    Col_name must be present in df unless\n",
    "    arg (table_name) is present in contribution\n",
    "    \"\"\"\n",
    "    table_name = arg\n",
    "    if col_name in df.columns:\n",
    "        return None\n",
    "    elif table_name in current_con.tables:\n",
    "        return None\n",
    "    else:\n",
    "        return \"{} column is required unless table {} is present\".format(col_name, table_name)\n",
    "\n",
    "    \n",
    "def requiredIfGroup(col_name, arg, dm, df):\n",
    "    \"\"\"\n",
    "    Col_name is required if other columns of \n",
    "    the group arg are present.\n",
    "    \"\"\"\n",
    "    group_name = arg\n",
    "    groups = set()\n",
    "    columns = df.columns\n",
    "    for col in columns:\n",
    "        if col not in dm.index:\n",
    "            continue\n",
    "        group = dm.loc[col]['group']\n",
    "        groups.add(group)\n",
    "    if group_name in groups:\n",
    "        if col_name in columns:\n",
    "            return None\n",
    "        else:\n",
    "            return \"{} column is required if column group {} is used\".format(col_name, group_name)\n",
    "    return None\n",
    "\n",
    "\n",
    "def required(col_name, arg, dm, df):\n",
    "    \"\"\"\n",
    "    Col_name is required in df.columns.\n",
    "    Return error message if not.\n",
    "    \"\"\"\n",
    "    if col_name in df.columns:\n",
    "        return None\n",
    "    else:\n",
    "        return '\"{}\" column is required'.format(col_name) \n",
    "\n",
    "def isIn(row, col_name, arg, dm, df):\n",
    "    \"\"\"\n",
    "    row[col_name] must contain a value from another column.\n",
    "    If not, return error message.\n",
    "    \"\"\"\n",
    "    #grade = df.apply(func, args=(validation_name, arg, dm), axis=1)\n",
    "    x = 0\n",
    "    cell_value = row[col_name]\n",
    "    if not cell_value:\n",
    "        return None\n",
    "    # if it's in another table\n",
    "    cell_values = [v.strip(\" \") for v in cell_value.split(\":\")]\n",
    "    if \".\" in arg:\n",
    "        table_name, table_col_name = arg.split(\".\")\n",
    "        if table_name not in current_con.tables:\n",
    "            return \"Must contain a value from {} table. Missing {} table.\".format(table_name, table_name)\n",
    "        if table_col_name not in current_con.tables[table_name].df.columns:\n",
    "            return '{} table is missing \"{}\" column, which is required for validating \"{}\" column'.format(table_name, table_col_name, col_name)\n",
    "        possible_values = current_con.tables[table_name].df[table_col_name].unique()\n",
    "        for value in cell_values:\n",
    "            if value not in possible_values:\n",
    "                return 'This value: \"{}\" is not found in: {}'.format(value, arg)\n",
    "                break\n",
    "    # if it's in the present table:\n",
    "    else:\n",
    "        possible_values = df[arg].unique()\n",
    "        for value in cell_values:\n",
    "            if value not in possible_values:\n",
    "                return 'This value: \"{}\" is not found in: {} column'.format(value, arg)\n",
    "                break\n",
    "    return None\n",
    "    \n",
    "def checkMax(row, col_name, arg, *args):\n",
    "    \"\"\"\n",
    "    row[col_name] must be less than or equal to arg.\n",
    "    else, return error message.\n",
    "    \"\"\"\n",
    "    cell_value = row[col_name]\n",
    "    if not cell_value:\n",
    "        return None\n",
    "    try:\n",
    "        arg_val = float(arg)\n",
    "    except ValueError:\n",
    "        arg_val = row[arg]\n",
    "    #arg = float(arg)\n",
    "    try:\n",
    "        if float(cell_value) <= float(arg_val):\n",
    "            return None\n",
    "        else:\n",
    "            #print \"{} must be <= {}\".format(str(cell_value), str(arg))\n",
    "            return \"{} ({}) must be <= {} ({})\".format(str(cell_value), col_name, str(arg_val), str(arg))\n",
    "    # this happens when the value isn't a float (an error which will be caught elsewhere)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def checkMin(row, col_name, arg, *args):\n",
    "    \"\"\"\n",
    "    row[col_name] must be greater than or equal to arg.\n",
    "    else, return error message.\n",
    "    \"\"\"\n",
    "    cell_value = row[col_name]\n",
    "    if not cell_value:\n",
    "        return None\n",
    "    try:\n",
    "        arg_val = float(arg)\n",
    "    except ValueError:\n",
    "        arg_val = row[arg]\n",
    "    try:\n",
    "        if float(cell_value) >= float(arg_val):\n",
    "            return None\n",
    "        else:\n",
    "            return \"{} ({}) must be >= {} ({})\".format(str(cell_value), col_name, arg_val, str(arg))\n",
    "    # this happens when the value isn't a float (an error which will be caught elsewhere)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def cv(row, col_name, arg, current_data_model, *args):\n",
    "    \"\"\"\n",
    "    row[col_name] must contain only values from the appropriate controlled vocabulary\n",
    "    \"\"\"\n",
    "    cell_value = row[col_name]\n",
    "    if not cell_value:\n",
    "        return None\n",
    "    cell_values = cell_value.split(\":\")\n",
    "    cell_values = [c.strip() for c in cell_values]\n",
    "    for value in cell_values:\n",
    "        if value.lower() in [v.lower() for v in vocabulary[col_name]]:\n",
    "            continue\n",
    "        else:\n",
    "            return '\"{}\" is not in controlled vocabulary for {}'.format(value, arg)\n",
    "    return None\n",
    "        \n",
    "\n",
    "# validate presence\n",
    "presence_operations = {\"required\": required, \"requiredUnless\": requiredUnless,\n",
    "                       \"requiredIfGroup\": requiredIfGroup, \n",
    "                       'requiredUnlessTable': requiredUnlessTable}\n",
    "# validate values\n",
    "value_operations = {\"max\": checkMax, \"min\": checkMin, \"cv\": cv, \"in\": isIn}\n",
    "\n",
    "def split_func(string):\n",
    "    \"\"\"\n",
    "    Take a string like 'requiredIf(\"arg_name\")'\n",
    "    return the function name and the argument:\n",
    "    (requiredIf, arg_name)\n",
    "    \"\"\"\n",
    "    ind = string.index(\"(\")\n",
    "    return string[:ind], string[ind+1:-1].strip('\"')\n",
    "\n",
    "\n",
    "def test_type(value, value_type):\n",
    "    if not value:\n",
    "        return None\n",
    "    if value_type == \"String\":\n",
    "        if str(value) == value:\n",
    "            return None\n",
    "        else:\n",
    "            return \"should be string\"\n",
    "    elif value_type == \"Number\":\n",
    "        try:\n",
    "            float(value)\n",
    "            return None\n",
    "        except ValueError:\n",
    "            return '\"{}\" should be a number'.format(str(value))\n",
    "    elif value_type == \"Integer\":\n",
    "        if isinstance(value, str):\n",
    "            if str(int(value)) == value:\n",
    "                return None\n",
    "            else:\n",
    "                return '\"{}\" should be an integer'.format(str(value))\n",
    "        else:\n",
    "            if int(value) == value:\n",
    "                return None\n",
    "            else:\n",
    "                return '\"{}\" should be an integer'.format(str(value))\n",
    "    else:\n",
    "        return None\n",
    "    #String, Number, Integer, List, Matrix, Dictionary, Text\n",
    "    \n",
    "\n",
    "\n",
    "def validate_df(df, dm):\n",
    "    # check column validity\n",
    "    cols = df.columns\n",
    "    invalid_cols = [col for col in cols if col not in dm.index]\n",
    "    for validation_name, validation in dm.iterrows():\n",
    "        value_type = validation['type']\n",
    "        if validation_name in df.columns:\n",
    "            output = df[validation_name].apply(test_type, args=(value_type,))\n",
    "            df[\"type_pass\" + \"_\" + validation_name + \"_\" + value_type] = output\n",
    "\n",
    "        val_list = validation['validations']\n",
    "        if not val_list or isinstance(val_list, float):\n",
    "            continue\n",
    "        for num, val in enumerate(val_list):\n",
    "            func_name, arg = split_func(val)\n",
    "            if arg == \"magic_table_column\":\n",
    "                continue\n",
    "            # first validate for presence\n",
    "            if func_name in presence_operations:\n",
    "                func = presence_operations[func_name]\n",
    "                #grade = func(validation_name, df, arg, dm)\n",
    "                grade = func(validation_name, arg, dm, df)\n",
    "                pass_col_name = \"presence_pass_\" + validation_name + \"_\" + func.__name__\n",
    "                df[pass_col_name] = grade\n",
    "    \n",
    "            # then validate for correct values\n",
    "            elif func_name in value_operations:\n",
    "                func = value_operations[func_name]\n",
    "                if validation_name in df.columns:\n",
    "                    grade = df.apply(func, args=(validation_name, arg, dm, df), axis=1)\n",
    "                    col_name = \"value_pass_\" + validation_name + \"_\" + func.__name__\n",
    "                    if col_name in df.columns:\n",
    "                        num_range = range(1, 10)\n",
    "                        for num in num_range:\n",
    "                            if (col_name + str(num)) in df.columns:\n",
    "                                continue\n",
    "                            else:\n",
    "                                col_name = col_name + str(num)\n",
    "                                break\n",
    "                    df[col_name] = grade.astype(object)\n",
    "    return df\n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# check that values pass validation\n",
    "# validation checks to add:\n",
    "# sv (suggested vocab)\n",
    "# requiredOneInGroup\n",
    "# requiredUnlessSynthetic\n",
    "\n",
    "\n",
    "# re-do upload_magic to use contribution-level (??)\n",
    "\n",
    "# first, do validations on each table in the contribution\n",
    "# this will include removing unneeded data (RmKeys from old upload_magic)\n",
    "# this will also include checking everything against the data model (strings are strings, etc.)g\n",
    "\n",
    "\n",
    "# next, splat out each table into a file and wrap it up.  give it a sensible name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#current_df = current_con.tables['sites'].df  \n",
    "#current_dm = current_con.tables['sites'].data_model.dm['sites']\n",
    "\n",
    "#current_df = validate_df(current_df, current_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_validation_col_names(df):\n",
    "    value_cols = df.columns.str.match(\"^value_pass_\")\n",
    "    present_cols = df.columns.str.match(\"^presence_pass\")\n",
    "    type_cols = df.columns.str.match(\"^type_pass_\")\n",
    "\n",
    "    value_col_names = df.columns[value_cols]\n",
    "    present_col_names = df.columns[present_cols]\n",
    "    type_col_names = df.columns[type_cols]\n",
    "\n",
    "    validation_cols = np.where(value_cols, value_cols, present_cols)\n",
    "    validation_cols = np.where(validation_cols, validation_cols, type_cols)\n",
    "    validation_col_names = df.columns[validation_cols]\n",
    "    return value_col_names, present_col_names, type_col_names, validation_col_names\n",
    "\n",
    "#value_col_names, present_col_names, type_col_names, validation_col_names = get_validation_col_names(current_df)\n",
    "#present_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# incorrect data type problems\n",
    "#current_df[type_col_names].dropna(how='all', axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# missing column problems\n",
    "#current_df[present_col_names].dropna(how='all', axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# value problems:\n",
    "#current_df[value_col_names].dropna(how='all', axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "canopy_exercise": {
     "cell_type": "<None>"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_col_name(string):\n",
    "    prefixes = [\"presence_pass_\", \"value_pass_\", \"type_pass_\"]\n",
    "    end = string.rfind(\"_\")\n",
    "    for prefix in prefixes:\n",
    "        if string.startswith(prefix):\n",
    "            return prefix[:-6], string[len(prefix):end]\n",
    "    return string, string\n",
    "\n",
    "\n",
    "def check_row(row):\n",
    "    ind = row[row.notnull()].index\n",
    "    values = row[row.notnull()].values\n",
    "    # to transformation with extract_col_name here???\n",
    "    return dict(zip(ind, values))\n",
    "\n",
    "#def check_row(row):\n",
    "#    return True\n",
    "\n",
    "\n",
    "def print_failures(failing_items, verbose=False, outfile=None):\n",
    "    if outfile:\n",
    "        ofile = open(outfile, \"w\")\n",
    "        ofile.write(\"\\t\".join([\"name\", \"row_number\", \"problem_type\", \"problem_col\", \"error_message\"]))\n",
    "        ofile.write(\"\\n\")\n",
    "    else:\n",
    "        ofile = None\n",
    "    for ind, row in failing_items.iterrows():\n",
    "        issues = row[\"issues\"]\n",
    "        string = \"{:10}  |  row number: {}\".format(ind, str(row[\"num\"]))\n",
    "        first_string = \"\\t\".join([str(ind), str(row[\"num\"])])\n",
    "        if verbose:\n",
    "            print first_string\n",
    "        #if outfile:\n",
    "        #    ofile.write(\"{}\\n\".format(string))\n",
    "        for key, issue in issues.items():\n",
    "            issue_type, issue_col = extract_col_name(key)\n",
    "            string = \"{:10}  |  {:10}  |  {}\".format(issue_type, issue_col, issue)\n",
    "            string = \"\\t\".join([issue_type, issue_col, issue])\n",
    "            if verbose:\n",
    "                print string\n",
    "            if outfile:\n",
    "                ofile.write(first_string + \"\\t\" + string + \"\\n\")\n",
    "    if outfile:\n",
    "        ofile.close()\n",
    "\n",
    "def get_all_failures(df, value_cols, type_cols, verbose=False, outfile=None):\n",
    "    df[\"num\"] = range(len(df))\n",
    "    # get column names for value & type validations\n",
    "    names = value_cols.union(type_cols)\n",
    "    # drop all non validation columns\n",
    "    value_problems = df[names.union([\"num\"])]\n",
    "    failing_items = value_problems.dropna(how=\"all\", subset=names)\n",
    "    if not len(failing_items):\n",
    "        if verbose:\n",
    "            print \"No problems\"\n",
    "        return []\n",
    "    failing_items = failing_items.dropna(how=\"all\", axis=1)\n",
    "    # get names of the failing items\n",
    "    bad_items = list(failing_items.index)\n",
    "    # get index numbers of the failing items\n",
    "    bad_indices = list(failing_items[\"num\"])\n",
    "    zip(bad_indices, bad_items)\n",
    "    #failing_items.drop(\"num\", axis=1, inplace=True)#.apply(check_row, axis=1).values\n",
    "    failing_items['issues'] = failing_items.drop(\"num\", axis=1).apply(check_row, axis=1).values\n",
    "    # maybe do a transformation in here so that you get \"lon\" instead of \"value_pass_lon_checkMax\"\n",
    "    print_failures(failing_items, verbose, outfile)\n",
    "    return failing_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_bad_rows_and_cols(df, validation_names, type_col_names, value_col_names, verbose=False):\n",
    "    df[\"num\"] = range(len(df))\n",
    "    problems = df[validation_names.union([\"num\"])]\n",
    "    all_problems = problems.dropna(how='all', axis=0, subset=validation_names)\n",
    "    value_problems = problems.dropna(how='all', axis=0, subset=type_col_names.union(value_col_names))\n",
    "    all_problems = all_problems.dropna(how='all', axis=1)\n",
    "    value_problems = value_problems.dropna(how='all', axis=1)\n",
    "    if not len(problems):\n",
    "        return None, None, None\n",
    "    #\n",
    "    bad_cols = all_problems.columns\n",
    "    prefixes = [\"value_pass_\", \"type_pass_\"]\n",
    "    missing_prefix = \"presence_pass_\"\n",
    "    problem_cols = []\n",
    "    missing_cols = []\n",
    "    long_missing_cols = []\n",
    "    problem_rows = []\n",
    "    for col in bad_cols:\n",
    "        pre, stripped_col = extract_col_name(col)\n",
    "        for prefix in prefixes:\n",
    "            if col.startswith(prefix):\n",
    "                problem_cols.append(stripped_col)\n",
    "                continue\n",
    "        if col.startswith(missing_prefix):\n",
    "            missing_cols.append(stripped_col)\n",
    "            long_missing_cols.append(col)\n",
    "            \n",
    "    if len(value_problems):\n",
    "        bad_rows = zip(list(value_problems[\"num\"]), list(value_problems.index))\n",
    "    else:\n",
    "        bad_rows = []\n",
    "    if verbose:\n",
    "        if bad_rows:\n",
    "            if len(bad_rows) > 20:\n",
    "                print \"-W- these rows have problems:\", bad_rows[:20], \" ...\",\n",
    "                print \"(for full error output see error file)\"\n",
    "            else:\n",
    "                print \"-W- these rows have problems:\", bad_rows\n",
    "        if problem_cols:\n",
    "            print \"-W- these columns contain bad values:\", \", \".join(problem_cols)\n",
    "        if missing_cols:\n",
    "            print \"-W- these required columns are missing:\", \", \".join(missing_cols)\n",
    "    return bad_rows, problem_cols, missing_cols\n",
    "    \n",
    "#a, b, c = get_bad_rows_and_cols(current_df, validation_col_names)\n",
    "#if a:\n",
    "#    print \"bad rows:\", a[:10]\n",
    "#    print \"problems:\", b[:10]\n",
    "#    print \"missing:\", c[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Validating locations\n",
      "Tel Hazor\t0\n",
      "value\tlat_s\t400.0 (lat_s) must be <= 90.0 (90)\n",
      "type\tlat_n\t\"hello\" should be a number\n",
      "Tel Megiddo\t1\n",
      "value\tlithologies\t\"random\" is not in controlled vocabulary for lithology\n",
      "-W- these rows have problems: [(0, 'Tel Hazor'), (1, 'Tel Megiddo')]\n",
      "-W- these columns contain bad values: lat_n, lat_s, lithologies\n",
      "-W- these required columns are missing: age_high, age_low, age, age_unit, dir_dec, dir_tilt_correction, geologic_classes\n",
      "-I- Complete list of row errors can be found in /Users/nebula/Python/PmagPy/locations_errors.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'locations'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run through and validate entire contribution\n",
    "\n",
    "#dir_path = os.path.join(os.getcwd(), '3_0', 'Megiddo')\n",
    "#con = Contribution(dir_path)\n",
    "the_con = con\n",
    "\n",
    "\n",
    "#for dtype in the_con.tables.keys()[2:4]:\n",
    "# doesn't work as validate_table function\n",
    "def validate_table(the_con, dtype, verbose=False):\n",
    "    \"\"\"\n",
    "    Return name of bad table, or False if no errors found\n",
    "    \"\"\"\n",
    "    print \"-I- Validating {}\".format(dtype)\n",
    "    # grab dataframe\n",
    "    current_df = the_con.tables[dtype].df\n",
    "    # grab data model\n",
    "    current_dm = the_con.tables[dtype].data_model.dm[dtype]\n",
    "    # run all validations (will add columns to current_df)\n",
    "    current_df = validate_df(current_df, current_dm)\n",
    "    # get names of the added columns\n",
    "    value_col_names, present_col_names, type_col_names, validation_col_names = get_validation_col_names(current_df)\n",
    "    # print out failure messages\n",
    "    ofile = os.path.join(os.getcwd(), \"{}_errors.txt\".format(dtype))\n",
    "    failing_items = get_all_failures(current_df, value_col_names, type_col_names, verbose, outfile=ofile)\n",
    "    #x = set(value_col_names).union(type_col_names)\n",
    "    bad_rows, bad_cols, missing_cols = get_bad_rows_and_cols(current_df, validation_col_names, \n",
    "                                                             value_col_names, type_col_names, verbose=True)\n",
    "    # delete all validation rows\n",
    "    current_df.drop(validation_col_names, axis=1, inplace=True)\n",
    "    if len(failing_items):\n",
    "        print \"-I- Complete list of row errors can be found in {}\".format(ofile)\n",
    "        return dtype\n",
    "    else:\n",
    "        print \"-I- No row errors found!\"\n",
    "        return False\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def validate_contribution(the_con):\n",
    "    passing = True\n",
    "    for dtype in the_con.tables.keys():#, 'criteria']:\n",
    "        print \"validating {}\".format(dtype)\n",
    "        fail = validate_table(the_con, dtype)\n",
    "        if fail:\n",
    "            passing = False\n",
    "        print '--'\n",
    "\n",
    "\n",
    "#validate_contribution(current_con)\n",
    "\n",
    "# work: sites\n",
    "# doesn't work: specimens, samples, locations\n",
    "validate_table(the_con, 'locations', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240.0\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.568\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "35.185\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>azimuth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hz05a</th>\n",
       "      <td>240.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05a</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05b</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05b</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05c</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05c</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05e</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05e</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05f</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05f</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05g</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05g</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz05h</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz06a</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz06a</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz06b</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz06b</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz06c</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz06c</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz07a</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz07a</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz07b</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz07b</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz07c</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz07c</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz07d</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz07d</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz07e</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz07e</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hz09a</th>\n",
       "      <td>35.568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq04t110</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq04t111</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq04t112</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq04t1PI</th>\n",
       "      <td>35.185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t101</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t102</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t103</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t104</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t105</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t106</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t107</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t108</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t109</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t111</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t112</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t1PI</th>\n",
       "      <td>35.185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t1PI</th>\n",
       "      <td>35.185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2PI</th>\n",
       "      <td>35.185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2PI</th>\n",
       "      <td>35.185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2a1</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2a2</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2b1</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2b2</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2c2</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2d1</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2d2</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2e1</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2e2</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2f1</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgq05t2f2</th>\n",
       "      <td>35.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               lon  azimuth\n",
       "sample                     \n",
       "hz05a      240.000      NaN\n",
       "hz05a       35.568      NaN\n",
       "hz05b       35.568      NaN\n",
       "hz05b       35.568      NaN\n",
       "hz05c       35.568      NaN\n",
       "hz05c       35.568      NaN\n",
       "hz05e       35.568      NaN\n",
       "hz05e       35.568      NaN\n",
       "hz05f       35.568      NaN\n",
       "hz05f       35.568      NaN\n",
       "hz05g       35.568      NaN\n",
       "hz05g       35.568      NaN\n",
       "hz05h       35.568      NaN\n",
       "hz06a       35.568      NaN\n",
       "hz06a       35.568      NaN\n",
       "hz06b       35.568      NaN\n",
       "hz06b       35.568      NaN\n",
       "hz06c       35.568      NaN\n",
       "hz06c       35.568      NaN\n",
       "hz07a       35.568      NaN\n",
       "hz07a       35.568      NaN\n",
       "hz07b       35.568      NaN\n",
       "hz07b       35.568      NaN\n",
       "hz07c       35.568      NaN\n",
       "hz07c       35.568      NaN\n",
       "hz07d       35.568      NaN\n",
       "hz07d       35.568      NaN\n",
       "hz07e       35.568      NaN\n",
       "hz07e       35.568      NaN\n",
       "hz09a       35.568      NaN\n",
       "...            ...      ...\n",
       "mgq04t110   35.185      0.0\n",
       "mgq04t111   35.185      0.0\n",
       "mgq04t112   35.185      0.0\n",
       "mgq04t1PI   35.185      NaN\n",
       "mgq05t101   35.185      0.0\n",
       "mgq05t102   35.185      0.0\n",
       "mgq05t103   35.185      0.0\n",
       "mgq05t104   35.185      0.0\n",
       "mgq05t105   35.185      0.0\n",
       "mgq05t106   35.185      0.0\n",
       "mgq05t107   35.185      0.0\n",
       "mgq05t108   35.185      0.0\n",
       "mgq05t109   35.185      0.0\n",
       "mgq05t111   35.185      0.0\n",
       "mgq05t112   35.185      0.0\n",
       "mgq05t1PI   35.185      NaN\n",
       "mgq05t1PI   35.185      NaN\n",
       "mgq05t2PI   35.185      NaN\n",
       "mgq05t2PI   35.185      NaN\n",
       "mgq05t2a1   35.185      0.0\n",
       "mgq05t2a2   35.185      0.0\n",
       "mgq05t2b1   35.185      0.0\n",
       "mgq05t2b2   35.185      0.0\n",
       "mgq05t2c2   35.185      0.0\n",
       "mgq05t2d1   35.185      0.0\n",
       "mgq05t2d2   35.185      0.0\n",
       "mgq05t2e1   35.185      0.0\n",
       "mgq05t2e2   35.185      0.0\n",
       "mgq05t2f1   35.185      0.0\n",
       "mgq05t2f2   35.185      0.0\n",
       "\n",
       "[244 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjust all degree columns to 360\n",
    "\n",
    "vals = ['lon_w', 'lon_e', 'lat_lon_precision', 'pole_lon', 'paleolon', 'paleolon_sigma', \n",
    " 'lon', 'lon_sigma', 'vgp_lon', 'paleo_lon', 'paleo_lon_sigma', \n",
    " 'azimuth', 'azimuth_dec_correction', 'dir_dec', 'geographic_precision', 'bed_dip_direction']\n",
    "\n",
    "\n",
    "def test_val(arg):\n",
    "    if not arg:\n",
    "        return arg\n",
    "    else:\n",
    "        print arg\n",
    "        try:\n",
    "            return float(arg) % 360\n",
    "        except ValueError:\n",
    "            return arg\n",
    "\n",
    "relevant_cols = list(set(vals).intersection(samp_df.columns))\n",
    "\n",
    "for col in relevant_cols:\n",
    "    samp_df[col] = samp_df[col].apply(test_val)\n",
    "samp_df[relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pmagpy import pmag\n",
    "reload(pmag)\n",
    "from pmagpy import data_model3 as data_model\n",
    "reload(nb)\n",
    "\n",
    "\n",
    "def upload_magic(concat=0, dir_path='.', dmodel=None):\n",
    "    \"\"\"                                                                                                                              \n",
    "    Finds all magic files in a given directory, and compiles them into an upload.txt file which can be uploaded into the MagIC datab\\\n",
    "ase.                                                                                                                                 \n",
    "    returns a tuple of either: (False, error_message, errors) if there was a problem creating/validating the upload file             \n",
    "    or: (filename, '', None) if the upload was fully successful                                                                      \n",
    "    \"\"\"\n",
    "    SpecDone=[]\n",
    "    locations = []\n",
    "    concat = int(concat)\n",
    "    files_list = [\"locations.txt\", \"samples.txt\", \"specimens.txt\", \"sites.txt\", \"ages.txt\", \"measurements.txt\",\n",
    "                  \"criteria.txt\", \"contribution.txt\", \"images.txt\"]\n",
    "    file_names = [os.path.join(dir_path, f) for f in files_list]\n",
    "    con = Contribution(dir_path)\n",
    "    # begin the upload process                                                                                                       \n",
    "    up = os.path.join(dir_path, \"upload.txt\")\n",
    "    if os.path.exists(up):\n",
    "        os.remove(up)\n",
    "    RmKeys = ['citation_label', 'compilation', 'calculation_type', 'average_n_lines', 'average_n_planes',\n",
    "              'specimen_grade', 'site_vgp_lat', 'site_vgp_lon', 'direction_type', 'specimen_Z',\n",
    "              'magic_instrument_codes', 'cooling_rate_corr', 'cooling_rate_mcd', 'anisotropy_atrm_alt',\n",
    "              'anisotropy_apar_perc', 'anisotropy_F', 'anisotropy_F_crit', 'specimen_scat',\n",
    "              'specimen_gmax','specimen_frac', 'site_vadm', 'site_lon', 'site_vdm', 'site_lat',\n",
    "              'measurement_chi', 'specimen_k_prime','specimen_k_prime_sse','external_database_names',\n",
    "              'external_database_ids', 'Further Notes', 'Typology', 'Notes (Year/Area/Locus/Level)',\n",
    "              'Site', 'Object Number']\n",
    "    print \"-I- Removing: \", RmKeys\n",
    "    CheckDec = ['_dec', '_lon', '_azimuth', 'dip_direction']\n",
    "    CheckSign = ['specimen_b_beta']\n",
    "    last = file_names[-1]\n",
    "    methods, first_file = [], 1\n",
    "    failing = []\n",
    "    if not dmodel:\n",
    "        dmodel = data_model.DataModel()\n",
    "    for File in file_names:\n",
    "        print \"-\"\n",
    "    # read in the data\n",
    "        Data, file_type = pmag.magic_read(File)\n",
    "        #df = nb.MagicDataFrame(File, dmodel=dmodel)\n",
    "        if file_type != \"bad_file\":\n",
    "            print \"-I- file\", File, \" successfully read in\"\n",
    "            container = con.tables[file_type]\n",
    "            df = container.df            \n",
    "            # drop non MagIC keys\n",
    "            DropKeys = set(RmKeys).intersection(df.columns)\n",
    "            df.drop(DropKeys, axis=1, inplace=True)\n",
    "            # make sure int_b_beta is positive\n",
    "            if 'int_b_beta' in df.columns:\n",
    "                df['int_b_beta'] = df['int_b_beta'].astype(float).apply(abs)\n",
    "                      \n",
    "            # make all declinations/azimuths/longitudes in range 0=>360.                                                     \n",
    "            #rec = pmag.adjust_all_to_360(rec)\n",
    "            df.columns.str.match()\n",
    "            #\n",
    "            # get list of location names\n",
    "            if file_type == 'locations':\n",
    "                locations = sorted(df['location'].unique())\n",
    "            \n",
    "            # run validations\n",
    "            res = validate_table(con, file_type)#, verbose=True)\n",
    "            if res:\n",
    "                failing.append(res)\n",
    "  \n",
    "            ## LJ: should we still do this??\n",
    "            if file_type == 'samples': # check to only upload top priority orientation record!                                      \n",
    "                NewSamps, Done = [], []\n",
    "                for rec in Data:\n",
    "                    if rec['sample'] not in Done:\n",
    "                        #orient,az_type=pmag.get_orient(Data,rec['sample'])\n",
    "                        #NewSamps.append(orient)\n",
    "                        Done.append(rec['sample'])\n",
    "            #    Data=NewSamps\n",
    "            #    print 'only highest priority orientation record from samples.txt read in '\n",
    "                \n",
    "            ## LJ: leave this for validations??\n",
    "            if file_type == 'specimens': #  only specimens that have sample names                                                   \n",
    "                NewData,SpecDone=[],[]\n",
    "                for rec in Data:\n",
    "                    if rec['sample'] in Done:\n",
    "                        NewData.append(rec)\n",
    "                        SpecDone.append(rec['specimen'])\n",
    "                    else:\n",
    "                        print 'no valid sample record found for: '\n",
    "                        print rec\n",
    "                Data = NewData\n",
    "                #print 'only measurements that have specimen/sample info'  \n",
    "                \n",
    "            ## LJ: leave this for validations??\n",
    "            if file_type == 'measurements': #  only measurements that have specimen names                                        \n",
    "                no_specs = []\n",
    "                NewData = []\n",
    "                for rec in Data:\n",
    "                    if rec['specimen'] in SpecDone:\n",
    "                        NewData.append(rec)\n",
    "                    else:\n",
    "                        print 'no valid specimen record found for: '\n",
    "                        print rec    \n",
    "                        no_specs.append(rec)\n",
    "                #print set([record['er_specimen_name'] for record in no_specs])                                                      \n",
    "                Data = NewData\n",
    "    # write out the data\n",
    "            if len(df):\n",
    "                container.write_magic_file(up, append=True)\n",
    "    # write out the file separator                                                                                                   \n",
    "            f = open(up, 'a')\n",
    "            f.write('>>>>>>>>>>\\n')\n",
    "            f.close()\n",
    "            print \"-I-\", file_type, 'written to ',up\n",
    "        else:\n",
    "            print 'File:', File\n",
    "            print file_type, 'is bad or non-existent - skipping '\n",
    "    ## add to existing file\n",
    "    if concat == 1:\n",
    "        f = open(up, 'a')\n",
    "        f.write('>>>>>>>>>>\\n')\n",
    "        f.close()\n",
    "     \n",
    "    if not os.path.isfile(up):\n",
    "        print \"no data found, upload file not created\"\n",
    "        return False, \"no data found, upload file not created\", None\n",
    "\n",
    "    #rename upload.txt according to location + timestamp                                                                             \n",
    "    format_string = \"%d.%b.%Y\"\n",
    "    if locations:\n",
    "        locs = set(locations)\n",
    "        locs = sorted(locs)[:3]\n",
    "        #location = locations[0].replace(' ', '_')\n",
    "        locs = [loc.replace(' ', '-') for loc in locs]\n",
    "        location = \"_\".join(locs)\n",
    "        new_up = location + '_' + time.strftime(format_string) + '.txt'\n",
    "    else:\n",
    "        new_up = 'unknown_location_' + time.strftime(format_string) + '.txt'\n",
    "\n",
    "    new_up = os.path.join(dir_path, new_up)\n",
    "    if os.path.isfile(new_up):\n",
    "        fname, extension = os.path.splitext(new_up)\n",
    "        for i in range(1, 100):\n",
    "            if os.path.isfile(fname + \"_\" + str(i) + extension):\n",
    "                continue\n",
    "            else:\n",
    "                new_up = fname + \"_\" + str(i) + extension\n",
    "                break\n",
    "    if not up:\n",
    "        print \"-W- Could not create an upload file\"\n",
    "        return\n",
    "    os.rename(up, new_up)\n",
    "    print \"Finished preparing upload file: {} \".format(new_up)\n",
    "    ## LJ: add back in validation stuff\n",
    "    if failing:\n",
    "        print \"-W- validation of upload file has failed.\"\n",
    "        print \"These tables have errors: {}\".format(\", \".join(failing))\n",
    "        print \"Please fix above errors and try again.\"\n",
    "        print \"You may run into problems if you try to upload this file to the MagIC database.\"\n",
    "    #    return False, \"file validation has failed.  You may run into problems if you try to upload this file.\", errors\n",
    "    else:\n",
    "        print \"-I- Your file has passed validation.  Yay!\"\n",
    "    return new_up, '', None\n",
    "\n",
    "\n",
    "dir_path = os.path.join(os.getcwd(), '3_0', 'Megiddo')\n",
    "#dir_path = os.path.join(os.getcwd(), '3_0', 'Osler')\n",
    "#dir_path = os.path.join(os.getcwd(), '3_0', 'McMurdo')\n",
    "\n",
    "upload_magic(dir_path=dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in an existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep all of df1, add in any extra from df2\n",
    "df1 = pd.DataFrame(np.random.randint(1, 10, (3, 5)), columns=['one', 'two', 'three', 'four', 'five'])\n",
    "df1.iloc[0, 1] = np.nan\n",
    "df1.iloc[2, 2] = np.nan\n",
    "df2 = pd.DataFrame(np.random.randint(1, 10, (3, 5)), columns=['one', 'three', 'five', 'seven', 'nine'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_df2_cols = df2.columns.difference(df1.columns)\n",
    "unique_df2 = df2[unique_df2_cols]\n",
    "\n",
    "# this adds in all the unique columns that weren't in df1\n",
    "concat_df = pd.concat([df1, unique_df2], axis=1)\n",
    "# fills in null values in df1 with values from df2\n",
    "concat_df.fillna(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! rm *_errors.txt\n",
    "! rm ./3_0/McMurdo/McMurdo*.txt\n",
    "#! rm ./3_0/Megiddo/Tel-Hazor*.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
